Okay, so now let's talk about Amazon ECS,

and we're gonna get an overview

into all different aspects of it.

So the first thing I wanna talk to you about

is the EC2 Launch Type.

So ECS stands for Elastic Container Service.

And when you launch Docker Containers on AWS,

you are launching what's called an ECS Task on ECS Cluster.

And an ECS Cluster is made of things.

And with the EC2 Launch Type,

well these things are EC2 instances.

And in that case,

if you use an ECS Cluster with an EC2 Launch Type

you must provision and maintain the infrastructure yourself.

So that means that your Amazon ECS/ ECS Cluster

is going to be composed of multiple EC2 instances.

Now, these instances are a little bit special

because each of them must run the ECS Agent,

and then this Agent is going to register each,

EC2 Instance into the Amazon ECS service

and the specified ECS Cluster.

Now, once you have that in place

then when you start ECS tasks

then AWS is going to be starting or stopping the containers.

That means that whenever we have a new Docker container

it's going to be placed accordingly

on each EC2 Instance over time as you can see right here.

And you can start or stop the ECS task,

and it will be placed automatically.

So that's the EC2 Launch Type,

and Docker containers are placed on Amazon

EC2 instances that we provision in advance, okay?

Now, there's a second launch type called

the Fargate Launch Type.

And again, you launch Docker containers on AWS

but this time you do not provision the infrastructure

so there are no EC2 instances to manage it.

It's all serverless.

Well, because we don't manage servers

but there of course, there are servers behind.

So, in the Fargate type,

if we have an ECS Cluster

we just create task definition to define our ECS tasks.

And then AWS will run these ECS tasks for us

based on how many CPU and RAM we need.

So when we want to run a new Docker container,

simple as that, it's going to be run,

without us knowing where it's run

and without an EC2 Instance to be created

in the backend in our accounts for it to work.

So it's a little bit magic.

And then to scale, well you just need to increase

the number of tasks.

Simple, you don't need to manage any more

EC2 instances.

And the exam loves to go

and tell you to use Fargate because Fargate is serverless,

and it's way easier to manage than the EC2 Launch Type.

Okay, so we've seen the two launch types for Amazon ECS.

Now let's talk about the IAM Roles for ECS tasks.

So let's take an example of the EC2 Launch Type

in which we have an EC2 Instance

running the ECS Agent on Docker.

So in this case, we can create an EC2 Instance Profile

which is only valued of course

if you use EC2 Launch Type.

And it's going to be used by the ECS Agents only,

and then the ECS Agent will use the EC2 Instance Profile,

to make API calls to the ECS service

to restore the instance,

is going to make API calls to CloudWatch Logs

to send container logs.

It's going to use the API calls to ECR,

to pull Docker images from ECR

and also reference sensitive data

in Secrets Manager or the SSM Parameter Store.

And then our ECS tasks are going to get ECS Task Roles.

And so this is valued for both

EC2 Launch Type and Fargate.

And so here I have two tasks.

And we can create a specific role per task.

So my first Task A will have an ECS Task A Role,

and the first Task B and second Task B

is going to have the Task B Role.

Well, why do we have different roles?

Because each role allows you to be linked

to different ECS services.

And so, for example, the ECS Task A Role

allows you to have your Task A,

runs some API calls against Amazon S3.

Whereas Task B Role allows you to run,

again API calls against DynamoDB.

And you define the Task Role

in the task definition of your ECS service.

So remember this, the distinction between

EC2 Instance Profile Role and the ECS Task Role.

Next, Load Balancer Integrations.

So in example, I'm in the EC2 Launch Type

but it could be Fargate as well, of course,

and have multiple ECS Tasks running.

It's all in the ECS Cluster.

And we want to expose these tasks

as a HTP or HTTPS endpoint.

Therefore we can run an Application Load Balancer

in front of it and then our users will be going

to the ALB and in the back end to the ECS tasks directly.

So in that case

the ALB is supported and will support most use cases,

and that's a good choice.

The Network Load Balancer is recommended only

if you have very high throughput

or high performance use cases,

or as you learn later on in this course,

if you use it with AWS Private Link.

Or, if you want to use the

older generation Classic Load Balancer you can,

but it's definitely not recommended

because you don't get any advanced features

and you cannot link your Elastic Load Balancer to Fargate.

Whereas if you're using the Application Load Balancer

then it works of course, with Fargate.

So what about data persistent on Amazon ECS?

For this you need Data Volume,

and they're different kinds but one of them

is noticeable and that's EFS.

So say you have an ECS cluster,

and in this case are represented both the EC2 Instance

as well as the Fargate Launch Type for my ECS Cluster.

And we want to mount a file system

onto the ECS task to share some data.

In that case, we use an Amazon EFS file system,

because it's a network file system is going to be compatible

with both EC2 and the Fargate launch types.

And it allows us to mount the file system directly

onto our ECS tasks.

Why? Well then tasks running in any

AZ linked to this Amazon EFS file system

will share the same data,

and therefore can communicate with another

via the file system if they wanted to.

So the ultimate combo,

is to use Fargate to launch ECS task

in the serverless fashion

and Amazon EFS for file system persistent,

because EFS again is also serverless,

we don't manage any servers, it's pay as you go.

It's just provisioned in advance and you're good to go.

So the use cases of using EFS with ECS

is to do persistent multi-AZ shared storage

for your containers.

# Creating ECS Cluster - Hands On.

So let's practice using Amazon ECS

by going into the ECS console service,

and then we're going

to enable the new ECS experience on the top left.

Then you're go in to cluster and we're going

to create our first cluster.

So this one is going to be called DemoCluster,

and we'll leave the default namespace as is.

So for infrastructure, we have three options.

The first one is AWS Fargate, which allows us

to provide the containers to AWS,

and then AWS will run these containers for us on demand.

So this is good because this is a serverless options.

We don't provide the compute.

AWS will provide the compute for us on demand,

but we don't even see it.

The other options are Amazon EC2 instances

where we provide our own Amazon EC2 instances

to provide the compute to run our containers on.

And we have external instances using ECS anywhere

where we can, for example, use your own data center

to run ECS containers on it.

So Fargate will leave it enabled,

and we'll also enable Amazon EC2 instances for the demo.

So we're going to create a new auto scaling group,

and then we have to choose the operating system.

So Amazon Linux 2 is great,

or you can choose the newest one, Amazon Linux 2023,

whatever you want.

For EC2 instance type, I'm going to use a t2.micro,

which is free tier eligible.

And then for desired capacity, I will choose minimum zero

and maximum five.

So no SSH key pair is going to be configured

and we'll leave the root EBS volume size as is.

Okay, now that we have specified an Amazon EC2 instance type

of infrastructure, we have to provide network settings

for the Amazon EC2 instances, so which is a default VPC,

as well as the three subnets available to me.

And for security group,

we'll use an existing security group

being the default security group.

And finally, for auto-assign public IP, again,

we'll use the default to use subnet setting.

We're not touching monitoring and tags.

And then we click on Create to create our cluster.

So while this is happening,

let's have a look at the auto scaling group

that is being created on AWS.

And then on the left-hand side,

I will click on auto scaling groups right here.

And it's showing, as you can see,

there was an auto scaling group created

for me called Infra-ECS-Cluster.

And desired capacity is zero, min capacity, zero,

max capacity, five.

And this was created directly by my ECS cluster.

And the creation is in progress.

And what this cluster has will have maybe EC2 instances

for me to launch tasks on.

So as you can see, it's in three availability zones.

So we know that the ECS tasks are going

to be launched across three AZ.

And what I'm going to do now is just wait for this cluster

to be created, which is the case now.

So what I can do now is explore this DemoCluster.

So if I click on it, I'm in the DemoCluster

and I can go to services are zero, tasks are zero

because we haven't launched anything yet.

And then we go to the more interesting infrastructure.

So if you go here to the infrastructure,

we have three capacity providers in this ECS cluster.

The first one is FARGATE.

That means that we can launch Fargate tasks

onto our ECS cluster.

The second one is FARGATE_SPOT

That means that we can launch Fargate tasks in spot mode,

select spot instances for EC2.

And the last one is a ASGProvider.

And this one means that we can launch EC2 instances

in this cluster directly through this ASG.

So it's managed scaling right now,

and the current size is zero,

but I can change it if I wanted to.

So let me show you what it would look like.

I go here, I go to details,

and I would edit the desired capacity to be one,

just to show you what it's like.

So what's going to happen out of this

is that an EC2 instance is going to be created, okay?

And when it's created,

it's going to register itself into the DemoCluster,

and then I will see it under container instances.

That means that when we create an ECS task,

it can either be launched on a FARGATE

or FARGATE_SPOT capacity provider,

or it can be launched on the container instances

that I will have launched as part of this ASG.

So what I'm going to do now is just wait for this instance

to be in the running state

and registered into my ECS cluster.

So let me refresh now.

My instance is InService and it's t2.micro.

And if I go back into my Amazon ECS cluster,

it is registered as a container instance that is currently,

of course, running zero tasks and has 1,024 CPU available

and 982 memory available.

So this is giving me the capacity of this instance,

and I can launch, as you'll see, tasks on it

until the capacity runs out.

So we're good to go.

So we have an ECS cluster.

We've seen two capacity providers, I mean three.

We've seen the container instances.

So now let's go ahead and do run our first service,

and I will see you in the next lecture to do that.

# Creating ECS Service - Hands On.

Now let's create an ECS service.

But before we do so, we need to create a task definition.

So I'm going to create a new task definition

from the task definition panels

and I need to give it a name.

So I'll call this one nginxdemos-hello.

And this is coming from this Docker image

called nginxdemos/hello on Docker Hub.

And so this is the one we're going to be using in our demo.

So this is why I call my task definition nginxdemo

with a hyphen of hello.

Okay, next we need to choose

the infrastructure requirements.

So do we want to launch on Fargate or Amazon EC2 instances.

Well, Fargate is serverless compute

so we'll leave it enabled.

And if we enable this, we could launch this task,

this service on our Amazon EC2 instances.

But for simplicity's sake right now,

I'm just going to use AWS Fargate

and launch our containers into serverless compute mode.

Then we need to choose what type of OS

and architecture we have.

So Linux is fine.

And what is the task size for our Fargate container?

So we can say for example, that we have 0.5 or 1 vCPU

and you can go up to 16 vCPU in this example.

And then you can also adjust memory.

So you can say, hey, I want up to, for example,

120 gigabytes of memory.

So all of this would be provided

by Fargate in a serverless fashion.

To keep things very cheap and simple,

I will choose 0.5 vCPU, and one gigabytes.

Next we have task role.

So task role is an IAM role that we can assign to our task

if we wanted to make API request to AWS services.

But because we don't do this right now,

we are not going to specify a task role.

But this is something of utmost importance

if your containers need to use AWS.

Now for the task execution role, leave it as default

and if this ECS task execution role is not created yet

it's going to be created automatically by ECS service.

So we're good.

Next our container, so the name is going to be

nginxdemos-hello and the image URL

is going to be nginxdemos/hello.

And this is going to pull automatically this image

from the Docker Hub right here.

So this is very handy and it's an essential container.

Now we have lots of different options.

For example, the port mappings.

So we want to map the port 80 to the port 80

of the container, which is great and we'll leave it as is.

And then you could add more port mappings if you wanted to.

You could, for example, set the resource allocation limits,

the environment variables from file or manually

and the logging.

But all these things I'm gonna leave as default

because this is going to work fine for us.

Storage, Fargate comes with some ephemeral storage.

So we'll leave it as is, again, from 21 gigabytes

which is a default right here.

And this is fine, just leave the value you have right now.

So let's create this.

And this is creating our first task definition.

Now you see version two for me

because I've just created it twice.

But for you, you should see version one.

So next, let's launch this task definition as a service.

So let's go into Clusters and then DemoCluster.

And under services I'm going to create a service.

So for compute option, I'm going to go into Launch type

and choose Fargate as my launch time

and the platform version to be latest.

And then the application type is a service.

That means it's going to be a long running service,

for example, a web application.

But if you wanted to have a task that terminates

and done, for example, a batch job, then you could use task.

But we'll use service

and then we'll select the nginxdemo-hello family

and choose whatever revision is the latest for you.

Should be number one for you.

Then let's assign a name for this service,

so nginxdemos-hello, the same as our task definition,

it's pretty standard.

And then the service type is going to be replica

of one task.

For deployment option, leave it as is

and deployment failure detection, leave it as is as well.

Next for networking, so we want to deploy into our VPC

on these three subnets

and we are going to create a new security group.

I will call this one nginxdemos-hello.

And I'll call this one security group for NGINX.

Now the idea is that we want to allow on HTTP

on port 80 and the source right now is going to be anywhere.

And for public IP, I leave this turned on.

And for load balancing, yes, we do want a load balancer.

It is going to be an application balancer.

You're going to create it.

So DemoALBForECS is the name,

we'll leave it as zero as the health check grace period.

We should leave enough time for our containers to start.

And then our containers is going to be this one

on port 80 on protocol HTTP.

And the target group is going to be tg-nginxdemos-hello.

The protocol is HTTP,

the protocol health check is HTTP

and the health check path is slash.

So now we've defined our service

where we have an application balancer, a target group,

a security group, and so on,

so we should be good to go.

We're not going to touch anything into service auto scaling

and task placement, but these things are available.

That means that, of course,

you can scale automatically on ECS.

So now my deployment is in progress

and we need to wait a few minutes.

So our service has now been deployed successfully.

So let's click on the service and have a look at it.

So we can see right now we have one desired task

and one is running and the status is active.

So this is really good.

And we can see that the service is linked to a target group.

So I click on the target group

and in the target group itself

we can see that it's linked to our DemoALB

which is the application balancer that was created

as part of this service.

And it looks like one IP address is registered as a target.

And this is the IP address of my container

which is very good.

So now if we have a look at this load balancer itself,

it is active, I can copy the DNS name

and then open a new tab and paste it.

And I get the Nginx welcome page, which is very good.

So that means that everything is working.

The server address is the exact same

as the IP we have registered in here.

So this is the private IP, which is good.

And what else?

So if we go under the service itself now

we can have a look at tasks.

So as you can see, one container is running right now

and this is this one task.

And I can click on it and have a look at this task itself.

So it tells me the configuration, the task revision,

where it's been launched, the private IP, the containers.

We can have a look at logs to know the logs

of our Nginx container as well, which is good.

And if we look at the service itself,

so we are on the service and then we go to events,

we can have a look at where the events of this.

So that means that we have one task that has been started.

It was registered in target-group

and then it's been complete deployment

and now we have a steady state.

So as we can see in this,

I can go to /test, for example,

and the URI will change in here.

So Nginx is working as expected.

Now what we can do because we're under ECS,

is that we have a look at our task,

we have one of them, but we can launch some more.

So I want to show you how easy it is

to launch more tasks with Fargate.

So let's update this service.

And now the desired number of task is going to be three.

So one per AZ, for example,

and the rest I will just leave as is.

So we'll leave the task definition to the same.

We'll leave the compute configuration as Fargate

and load balancing does not change

in terms of health checks and so on.

So let's just click on Update.

And now what's going to happen is that we have asked

the ECS service to run two more tasks.

So if I refresh this and wait a little bit,

now we have two more tasks being provisioned

and they are provisioned on the Fargate engine.

So that means that behind the scenes,

AWS is going to provision automatically the resource

that it needs to launch these tasks.

So let's wait a little bit.

They're pending, now they're activating

and now they're running.

So this was very quick actually.

And if I go under here

and now refresh this page, as you can see

the IP address is changing every time I refresh.

So the application balancer is distributing the load

between all my containers on ECS, which is great.

So this is quite powerful

and we just demonstrated the scaling

of ECS while scaling up.

And just to scale back the demo and save on cost,

we can update the service here

and have the desired number of tasks to be zero.

This way the service is still there

but we are don't have any containers running.

And under my application load balancer,

my auto scaling group, sorry,

then I'm going to click on this

and make sure the desired capacity is also zero.

This way we are sure not to be running any type

of instances on our EC2 cluster for ECS, okay?

So now you can verify this

that the tasks are gone and you're good to go

and you can look at the events to see what has ECS done

while we were asking you to update the service.

Okay? So that's it.

We've seen how to create an ECS cluster,

we've seen how to create an ECS service on Fargate.

I hope you liked it and I will see you in the next lecture.

# Amazon ECS - Auto Scaling.

So now let's talk

about ECS Service Auto Scaling.

So as we can see, we can manually increase

the number of ECS tasks in our service.

But we can also automatically increase

or decrease the number of tasks.

For this, we can leverage the service

called the AWS Application Auto Scaling.

And we have three metrics we can scale on using the service.

We can scale on the CPU Utilization of the ECS Service.

We can scale on the Memory Utilization,

which is the RAM of the ECS Service.

Or, the ALB Request Count Per Target,

which is a metric coming from the ALB.

So only these metrics you have to remember.

Then, you can set up different kind of auto scaling.

You can set up Target Tracking to track

for a specific target for the three metrics shown above.

Or Step Scaling.

Or Scheduled Scaling,

if you wanted to scale your ECS Service ahead of time

thanks to predictable changes.

Remember, that scaling your service, your ECS Service,

at the task level is not equal to scaling your cluster

of EC2 instances if you are in the EC2 launch type.

And so therefore, that's why when you don't

have an EC2 auto scaling that's necessary,

when you don't have EC2 instances in the backend,

then using Fargate makes service auto scaling much easier

to set up, because everything is serverless.

And it's why I'm fan of Fargate.

And the exam is pushing you to use Fargate a lot.

So for the EC2 launch type,

how can we actually scale the EC2 instances

in the backend if we're using it?

So we have multiple ways of doing it.

We can use an Auto Scaling Group Scaling.

And so we scale our ASG, for example,

based on CPU Utilization.

And then we can add EC2 instance over time

if the CPU skyrockets.

Or, we can use the newer and more advanced feature

called the ECS Cluster Capacity Provider

that we've seen before in the hands-on.

And with this one, the Capacity Provider is very smart.

And as soon as you lack capacity to launch new tasks,

it's going to automatically scale your ASG.

So the Capacity Provider is paired

with a Auto Scaling Group.

And then when you're missing RAM or CPU,

there you go, EC2 instances are created.

And so the second option is the smarter way of doing things.

So if you have to choose between Auto Scaling Group Scaling

and ECS Cluster Capacity Provider,

please use ECS Cluster Capacity Provider

for your EC2 launch type.

So let's have a look at the service.

So we have a Service A with two tasks.

And we have CPU Usage.

And it's going to be auto scaled

by the AWS Application Auto Scaling.

But let's assume we have more users,

and therefore your CPU usage goes really up,

then your CloudWatch Metric,

which monitors the CPU Usage at the ECS service level again,

is going to trigger a CloudWatch Alarm,

which will trigger a scaling activity

in your Auto Scaling for your ECS service.

The desired capacity will increase

for your ECS Service, and a new task will be created.

And optionally, if this service is running

on the EC2 launch type,

then the ECS Capacity Providers

can help you scale your ECS cluster

backed by EC2 instances.

# Amazon ECS - Rolling Updates.

Now let's talk about

how do you update an ECS service.

And for this we have rolling updates.

So when you update an ECS service from v1 to v2,

you can control how many tasks will be started

and stopped at a time and in which order.

So when you have an ECS updates,

when you select a new task definition number

and you want to update an ECS service,

you will have two settings,

the minimum healthy percent and the maximum percent.

So by default they're one and 200

but let's see what they mean.

So your ECS service,

for example, this one is running nine tasks

represents an actual running capacity of 100%.

And then if you set a minimum healthy percent

of less than 100, this is going to say,

"Hey you're allowed to terminate all the tasks

on the right hand side, as long as we have enough tasks

to have a percentage over the minimum healthy percent."

And in the maximum percent shows you how many new tasks

you can create of the version two,

to basically roll updates your service.

So this is how these two settings would impact your updates.

And so you will go ahead, create new tasks,

then terminate all tasks and so on.

All to make sure that all your tasks

are going to be terminated

and then updated to a newer version.

So let's discuss two scenarios.

For example, we have min at 50%

and max at 100% and we start with four tasks.

In this case,

we're going to lose four tasks to be terminated

so that we're running at 50% capacity.

Then two new tasks are going to be created, okay?

Now we're back at 100% capacity.

Then two old tasks are going to be terminated,

we're back at 50% capacity.

And two new tasks are going to be created,

we're back at 100 capacity.

And we have done a rolling updates.

In this case, we have been terminating tasks

because we set the minimum to 50%

and the maximum to 100%.

If we're doing minimum at 100% and maximum at 150%

then we start with four tasks.

We cannot determine a task because the minimum is 100%.

Therefore we can go into create two new tasks

and this will bring our capacity to 150%.

Then because we are above the minimum 100%

we can terminate two old task and we're back at 100%.

Then we will create two new tasks

and finally, terminates two old tasks.

And this will have performed our rolling updates

for our ECS service.

So just something you should know,

if you read lecture going into the exam

this could be happening at one question only.

All right, that's it.

I will see you in the next lecture.

# Amazon ECS - Solutions Architectures.

Now let's talk about

a few solution architectures

you can encounter with Amazon ECS.

So the first one are ECS tasks invoked by Event Bridge.

So for example, say we have an Amazon ECS cluster,

it's backed by Fargate, and we have S3 buckets.

Our users are going to upload objects into our S3 buckets,

And these S3 buckets can be, for example,

integrated with Amazon Event Bridge

to send all the events to it.

And Amazon Event Bridge can have a rule

to run ECS tasks on the go.

Now, when ECS tasks are going to be created,

they will have an ECS task role associated with them,

and from the task itself

what it can do is that it can get the objects, process it,

and then send the results into Amazon DynamoDB.

And that is thanks to the fact

that we have an ECS task role associated with it.

And so effectively here, what we've done

is that we've created a serverless architecture

to process images, or to process objects,

from your S3 buckets using a Docker container.

And that is using Amazon Event Bridge ECS

in the Fargate mode, as well as an ECS task role

to talk to Amazon S3 and Amazon DynamoDB.

Another architecture using, again, Event Bridge,

is to use an Event Bridge schedule.

So we have an Amazon ECS cluster

backed by Fargate and Amazon Event Bridge,

and we schedule a rule to be triggered every 1 hour.

Now, this rule is going to run ECS tasks for us in Fargate,

and so that means that every 1 hour,

a new task will be created in our Fargate cluster,

and the task can do whatever we want.

For example, we can create an ECS task role

with access to Amazon S3, and therefore our task,

our Docker container, our program can, for example,

do every 1 hour some batch processing

against some files in Amazon S3.

And again, all of that architecture is fully serverless.

A last example is using ECS and an SQS queue,

so we could have a service on ECS with two ECS tasks,

and messages are being sent into an SQS queue,

and the service itself is pulling for messages

from the SQS queue, and processing them.

We can enable ECS Service Auto Scaling

on top of this service.

That means that, for example, the more messages we have

in our SQS queue, the more tasks we're going to have

into our ECS service, thanks to auto-scaling.

Another integration is when you want to use Event Bridge

to actually intercept events from within your ECS cluster.

So, for example, say you wanted to react

to tasks being exited.

In that case, any task exiting or starting

in your ECS cluster can be triggered as an event

in Event Bridge, and it will look like this.

For example, the ECS task state change for "stopped"

and the stopped reason.

Then from there, for example, we could alert an SNS topic

and send emails to your administrators.

So, bottom line, Event Bridge does allow you to understand

the lifecycle of your containers in your ECS cluster.

Okay, so that's it for this lecture.

I hope you liked it, and I will see you in the next lecture.

# Amazon ECS Task Definitions - Deep Dive.

Now let's talk about

Amazon ECS Task Definitions, but in depth.

So you define them in JSON form

but through the console

there is a UI to help you create the JSON,

and the task definition tells the ECS service,

how to run a or multiple Docker containers on ECS.

And there is crucial information

within your task definition, such as the Image Name,

the Port Binding for the Container

and the Host, if you're on EC2.

The memory and the CPU required for your container.

The environment variables, the networking information,

the IAM role attached to the task definition,

and the login configuration

such as, for example, CloudWatch.

So there's more information as well

but these are the most important

and the exam will test you on a few of these.

So, I will do a deep dive on some of that in this lecture.

So let's take an example.

We have an EC2 instance,

and because it is registered with an ECS cluster,

it has to be running the ECS agent.

Next, we're going to run a Docker container

through ECS test definition,

for example, an Apache HTP server.

And we have to expose that server to the internet.

Therefore, we are going to define a container port

on 80, meaning that on the container ,

the port 80 is the one that is exposing the HTP server.

But then we have also the host port

because we are on EC2.

If we were on Fargate, that would not be relevant,

but we are on EC2.

And therefore we need to map this container port

to a host port, which is for example,

could be 80 but it could be also at 8080.

So they don't have to be the same.

And then thanks to the host port, then the internet

or an external network communication

is able to access the EC2 instance,

on port 8080, the host port,

which is going to be directed to the container port 80,

and then will get access to the HTP server.

Okay.

And you should know that you can define

more than one container per task definition.

You can define up to 10 containers per task definition.

So let's do a deep dive onto first the container port.

So if you have load balancing

and you're using the EC2 launch type,

then you're going to get

what's called a Dynamic Host Port Mapping.

If you define only the container port

and the task definition.

Let me explain.

So we are running for example, an ECS task,

and all of them have the container port

set to 80 but the host port set to zero,

meaning not set.

What's going to happen is that the host port only

is represented in this diagram,

but the host port is going to be random,

is going to be dynamic.

And so, each ECS task

from within the EC2 instance,

is going to be accessible from a different port on the host,

the EC2 instance.

And therefore, if you define an application of that answer

then you may say, well,

it is difficult for the ALB to connect

to the ECS test because the port is changing.

But the ALB when linked to an ECS service

knows how to find the right port,

thanks to the Dynamic Host Port Mapping feature.

And so, the ALB,

automatically thanks to the ECS service,

knows to connect to different ports

onto different instances automatically.

And so this setup works,

but it does not work with a classic load balancer

because it is older generation.

So this logic only happens with the ALB.

And so therefore from a security perspective,

well the EC2 instance security group, must allow any port,

from the ALB security group,

because we don't know in advance

what is going to be the host port mapping.

So that was for the EC2 launch type.

But now what happens when we have the Fargate launch type?

Well, each ECS task is going to get

a unique private IP this time.

And so, because this is Fargate,

there is no host,

and therefore we only have to define the container ports.

And so if you look at your ECS cluster,

then for example, with four tasks

each task is going to get its own private IP

through an Elastic Network Interface or ENI.

And then each ENI is going to get

the same container ports.

And so this is a setup you're going to get with Fargate.

And therefore, when you have an ALB,

then to connect to the Fargate task,

it's just going to connect to all all of them

on the same port on port 80.

So there is the ECS ENI Security Group

that needs to allow port 80, from the ALB security group.

And then the ALB security group

needs to allow just to port 80 or 443,

if you have SSL enabled, from the web.

Next, the exam will ask you about IAM roles within ECS.

And you should know that IAM roles are assigned

per task definition.

So you have a task definition

and then you assign an ECS task role.

And this will allow you, for example,

for your ECS tasks out of your task definition,

to access the Amazon S3 service.

And therefore when you create an ECS service

from this task definition

then each ECS task automatically

is going to assume and inherit this ECS task role.

But you should know that the role is defined

at the task definition level, not at this service level.

And so, therefore all the tasks

within your service, are going to get access to Amazon S3.

And if you define another task definition,

you can add another role on it.

And this role, for example, can access DynamoDB.

And if you were to create another service,

then that service would assume this other role

and you would be good to go.

So the exam will ask you,

where do you define an IAM role for ECS task?

And the answer is, on your task definition.

Next, you have environment variables.

So, your task definition, can have environment variables

and they can come from multiple places.

You can hard code them.

For example, you set them directly

from within the test definition.

And for example,

this is when you will have a fixed non-secret URL.

But then if you have sensitive variables,

such API keys or shared configs,

or for example, database passwords,

then you can use either the SSM Parameter Store,

or Secrets manager, to store these values.

And you reference them from within the ECS task definition

and upon launching an ECS task

then these values are going to be fetched

and resolved at run time

and injected as environment variables, within your ECS task.

Finally, there is a last option

where you load your ECS environment variables

directly from an Amazon S3 bucket.

And this is called a bulk environment

variables loading through a file.

Next we have, how do we share data between ECS tasks?

So, as I said, an ECS task can contain one container

but also you can define multiple containers

in the same task definition.

And you would do that because sometimes,

your side containers also called side cars

can help you with lugging, with tracing and so on.

And so, it's a common pattern.

But sometimes, well for example, for lugging

and for metrics and so on,

these containers need to share some files together.

And therefore, we must mount a data volume

onto both containers,

and then they will be able to share data.

And so, this data volume, bind them out,

works for both EC2 and Fargate tasks.

So let's imagine we have an ECS task

and we have the application containers.

It could be one, it could be many.

And then some side car containers,

for example the metrics and logs container.

You're going to create a bind mount,

and it's going to create a shared storage

that you have to define between your task.

And you say, for example, it is /var/logs

and so therefore your application containers

are going to be able to write

to this shared storage,

and your metrics and log container

can read from these shared storage.

And this is the whole idea behind the bind mount.

So if you use EC2 tasks

then the bind Mount itself is the EC2 instant storage.

And therefore the data of that mount is tied

to lifecycle of the EC2 instance.

Or, for Fargate task, then you use ephemeral storage,

and the data is tied to the container,

a lifecycle using them.

And so whenever your Fargate task disappears,

then your storage disappears as well.

Now, on Fargate you get from 20 gigabytes to 200 gigabytes

of shared storage,

so it gives you lot of space for different use cases.

So between them.

So the basic use case,

especially from the exam perspective

is to share data between multiple containers

or when you have a side car container

where the side car is used to, for example

send metrics or logs to other destinations.

And it needs to read from a shared storage.

Okay.

So that's it for the Amazon ECS task definition, deep dive.

I hope you liked it.

And I will see you in the next lecture.

# Amazon ECS Task Definitions - Hands On.

So let's have a look at all the option

for creating a task definition.

So here is a family name, so I'll call this one wordpress.

And then we have to choose infrastructure requirements.

So we can launch on Fargate

and we can launch on Amazon EC2 instances.

It could be either or both.

And based on the options,

if it's Fargate then we have to choose CPU,

and memory increments are compatible with Fargate.

But if it's Amazon EC2 instances

then you can enter whatever value you want

for memory and CPU.

So that's one difference.

And of course, if you are on Fargate

then the network mode has to be AWS VPC.

But if you're only on EC2

then you have the option to choose

a more advanced network mode.

Just something to know about.

Then, task role are very important

because they allow your container

to make API calls to IWS services

and to get automatically an IAM role for this.

So this role is pretty big at the exam.

The task execution role is an IAM role

specifically for the container agent

to make AWS API requests on your behalf.

So this is more of a standard role for ECS,

whereas the task role is really the role

you're going to use for your ECS tasks.

Okay, next container.

So you can have as many containers as you want

in your ECS task definition,

but one of them at least has to be an essential container.

So that means you enter a name,

for example, wordpress,

and the image URI is WordPress.

And yes, this is an essential container.

And you can add more containers at the bottom.

As you can see right here, I have Container 2.

Now, if you set a container as essential

that means that if the container fails for whatever reason

and is killed,

then because this's an essential container

all the task is going to be stopped.

If this is not an essential container

then it can be stopped and the task will still go on.

So it depends on what your application does

to know whether or not a container is essential.

So now we have Container 1,

and it pulls an image from here and this URI.

And so, we can have the image

in the private registry if we want to.

And so, for this you need authentication.

And so, we just enter the Secrets Manager ARN

for this secret,

and we can pull these images from a secret repository,

from private repository instead of public one.

Next we have port mapping.

So this is where you define all the ports

for your containers,

and the protocols,

the port name, and the type of protocol we have.

So HTP, HTP2, GRPC, or none.

And you can add as many port mappings as you want,

which is very handy

if your application is exposing several ports.

Next, you have some files

around if you want to limit the number

of VCPUs, and memory,

and the hard and soft limits,

which makes sense if you have multiple containers

that run next to each other.

Now, for environment variables

you can add them individually.

So we can say, for example,

the FOO is equal to value BAR,

so this's the name and this is the value.

But also, we can have them from a secret manager.

So for example,

this one is going to be called SECRET_DB_PASSWORD.

That's the name of this environment variable.

And then we do value from,

and here you add the ARN of the secrets manager secret

that has this value,

the SECRET_DB_PASSWORD.

So this is for secrets manager,

but also for the SSM parameter store.

So this is when you set the environment variables

directly from here,

or you can add them from a file.

And this file is a file

that is going to be hosted on Amazon S3,

so good to know.

Now, for logging.

So logging can be log collection,

and this is done.

And you can send it to CloudWatch

or to Splunk, or Firehose,

or Kinesis Stream, or OpenSearch, or S3,

all via something called AWS FireLens

or directly to CloudWatch natively.

And if you use CloudWatch

then we have to specify the log group,

the log region, the stream prefix,

and whether or not we want to create the group.

But you can also add more log configuration values

if you wanted to.

HealthChecks make sure that your container is still healthy

if you wanted to.

Timeouts, make sure that you have

a start and a stop timeout.

Stop timeout you get killed

before it properly ends,

and start timeout is that if it doesn't start fast enough

then it gets killed.

We can set some Docker configuration,

Docker labels, and specific resources,

but these are less important.

So as we said, \we can have as many containers as needed,

essential or not.

Then we have storage.

So we can provide more storage for our containers.

So it could be a bind mount

or an EFS file system that we mount.

So bind mount is saying, "Hey, what's the volume name?"

And then we can add multiple volumes if we wanted to,

some of them would be EFS, for example.

It's up to you to how you wanna mount your data.

And then you have the container mount points that you define

and you say, "Okay, which path I wanna mount my volume on,"

and then where the volumes can be from.

It could be mounting a volume from another container.

So this's all described in detail in the documentation,

but what you have to remember is that you can mount data

from EFS or from a file system

directly onto your containers,

or from containers onto each other.

From monitoring, we can also enable trace collection

to send data into AWS X-ray

through a sidecar called

the AWS Distro for Open Telemetry.

And automatically the CPU and memory

will be adjusted to add for this sidecar.

And we can also collect metrics

and send them into CloudWatch

or Managed Service for Prometheus,

or Managed Service for Prometheus,

using different kinds of libraries.

And so, this is to send your metrics

into a central place if you wanted to.

So we've seen everything

in terms of how we configure our tasks.

And then you click on Create

and you can review all the settings

directly from the JSON,

if you wanted to have a look at the JSON itself.

Or if you wanted to have a look at the data itself

you would just create a new revision

and then, again, modify them one by one.

But we've seen how to do

a full task definition configuration using the console.

So I hope you liked it,

and I will see you in the next lecture.

# Amazon ECS - Task Placements.

So a new concept you need to know

for the exam is the concept of ECS tasks placements.

So when you create a task of type EC2,

ECS must figure out

where to place the task based on the available memory, CPU

and ports on your target EC2 instances.

So for example, here is our ECS cluster

made of three EC2 instances

and some tasks are placed on each instance

in some ways.

If the ECS service has a new container

a new task that it wants to place on your EC2 instances,

it needs to figure out where to place it.

And that's the big question mark.

So similarly, whenever a service scales in

I mean that we remove an ECS task,

the ECS service needs to determine

which ECS task to terminate.

Okay.

So to assist you with this

you can define what's called a task placement strategy

and task placement constraints.

And this will guide where this new container

will be added or where a container will be removed from.

This is only working for when you use ECS

launched on EC2 instances,

not for Fargate,

because for Fargate AWS figures out

for you where to start the container

and you don't manage any backend instances.

Okay.

So this is only valid for ECS on EC2.

Okay.

Now let's talk about the task placement process.

Okay.

So the first thing to note is that

task placement strategies are a best effort.

If you wanted to have constraints

we'll see this very very soon.

Okay.

So when ECS places tasks

it will use the following process

to select where to place it.

The first one is that it will identify instances

that satisfy the CPU, memory

and port requirements in the task definition.

So figure out where it can place the task

in the first place.

Then it will look at the

task placement constraints that we'll see in this lecture.

And then finally,

it will try to identify the instance

that satisfies best the task placement strategy.

And then it will select that instance for the task placement

and place the task there.

So now let's talk about what

these task placement strategies are.

The first one that you need to know for the exam that comes

up is called binpack.

And binpack will place tasks

based on the least available amount of CPU or memory.

And this is to help you minimize

the number of instances in use.

So this will bring you cost saving.

So if you want to define a binpack placement strategy

this is the JSON that's according to it.

So it's here.

It says binpack on the field memory so RAM.

And so that means that if we have one EC2 instance,

it's going to try to fill up

that EC2 instance all the way with containers.

And then when it can't put any more containers

on that one EC2 instance,

it's going to place EC2 containers

on another EC2 instance.

So as you can see here

we are placing as many containers as possible

on one EC2 instance before moving on

to the other.

That's why it's called binpack

because it packs all the containers together.

And that's the strategy that brings the most cost saving

because it will minimize the number of EC2 instances in use

and try to maximize the utilization

of one EC2 instance at a time.

Okay?

Next the task placement strategy for random.

So here very simple, it places the tasks randomly.

So if we look at this placement strategy

in JSON it's very simple.

And so say we have two EC2 instance

and tasks are being added

then they would just be placed randomly.

And there is no logic to it, just random,

very very simple.

Not an optimal strategy,

but one that works really great.

Okay.

The last placement version to be aware of

is called spread.

So say we have three EC2 instances

and they are in three different availability zones.

Then, if we do a spread

the task will be spread

based on the specified value.

So for example, that value could be instance ID

or ECS availability zones and so on.

So in this example

I'll make it very simple

and understandable.

We choose a placement strategy of spread

on the ECS availability zones.

That means that the task will be spread evenly

across AZs.

So my first task may be on AZ-A

then AZ-B,

then AZ-C.

And so they're spread as you can see on each AZ

and then it will start all over again.

So with this,

we have basically maximized the high availability

of our ECS service by spreading the task

on the EC2 instances.

So, these task placement strategies

they can be mixed together.

So we can have a spread on availability zone

and then a spread on instance ID

or we can have a spread on availability zone

and then a binpack on memory.

So it is possible for you to mix and match.

But the exam should test you only on the basics.

So understanding the difference

between binpack spread and random.

And then we have ECS task placement constraints

to bring constraints to how tasks are being placed.

So, the first one is called distinctInstance

and we are seeing through the ECS service

that each task should be placed

on a different container instance.

So you will never have two tasks on the same instance.

And this is defined by this JSON

called placement constraints distinctInstance.

And then there is a second task constraint

called memberOf

and we want to place task

on instances that satisfy an expression

that can be defined in the cluster query language,

which is pretty advanced.

And so I'll just give you one concrete example of it

and that should be enough going into the exam.

So for example, we do a placement constraint here

and we wanna say that the instance type must be of type t2.

And so what we are saying here is that

all these tasks should be placed only on t2 instances.

So this is the kind of constraints

you can use with a memberOf.

So we have distinctInstance

which is very easy to understand

and memberOf with a more complicated

cluster query language to force your task to be

for example only on EC2 instances.

Okay.

That's it for this lecture.

I hope you liked it.

And I will see you in the next lecture.


So, we have to talk about the S3 Baseline performance.

So, by default, Amazon S3 automatically scales

to a very, very high number of requests,

and has a very, very low S3 between 100 and 200 milliseconds

to get the first byte out of S3.

So, this is quite fast.

And in terms of how many requests per second you can get,

you can get 3,500 PUT/COPY/POST/DELETE,

per second, per prefix,

and 5,500 GET/HEAD requests

per second per prefix in the buckets.

So, this is something you can get on the website

and I think it's not very clear,

so I'll explain to you what per second per prefix means.

But what that means in viral is

that it's really, really high performance and,

there's no limits to the number of prefixes in your bucket.

So, let's take an example of four objects named file,

and let's analyze the prefix for that object.

The first one is in your bucket,

in folder one, sub folder one slash file.

In this case, the prefix is going to be anything

between the bucket and the file.

So, in this case it is slash folder one, slash sub one.

So, that means that for this file in this prefix,

you can get 3,500 Puts and 5,500 Gets per second.

Now, if we have another folder one and then sub two,

the prefix is anything between buckets and file,

so slash folder one slash sub two,

and so we get also 3,500 Puts and 5,500 Gets

for that one prefix, and so on.

So, if I have one and two we have different prefixes,

and so it's easy now to understand what a prefix is,

and so it's easy to understand the rule of 3,500 Puts

and 5,500 Gets per second per prefix in a bucket.

So, that means that if you spread reads

across all the four prefixes above evenly,

you can achieve 22,000 requests per second

for Head and Gets.

Now, let's talk about S3 performance,

how we can optimize it?

The first one is multi-part upload.

So, it is recommended to use multi-part upload

for files that are over 100 megabytes,

and it must be used for files that are over five gigabytes.

And what multi-part upload does is

that it parallelize uploads and that will help us

speed up the transfers to maximize the bandwidth.

So, as a diagram, it always makes more sense.

So, we have a byte file,

and we want to upload that file into Amazon S3.

We will divide it in parts, so smaller chunks of that files

and each of these files will be uploaded

in parallel to Amazon S3.

In Amazon S3, once all the parts have been uploaded,

it's smart enough to put them together

back into the big file.

Okay, very important.

Now, we have S3 transfer acceleration,

which is for upload and download

and it is to increase the transfer speed

by transferring a file to an AWS edge location,

which will forward the data to the S3 bucket

in the target region.

So, edge locations there are more than regions.

There are about over 200 edge locations today,

and it's growing,

and let me show you in the graph what that means?

And that's S3 transfer acceleration

is compatible with multi-part upload.

So, let's have a look.

We have a file in the United States of America,

and we want to upload it to S3 bucket in Australia.

So, what this will do is that we will upload that file

through an edge location in the United States,

which will be very, very quick,

and then we'll be using the public internet.

And then from that edge location

to the Amazon S3 bucket in Australia,

the edge location will transfer it

over the fast, private AWS network.

So, this is called transfer acceleration,

because we minimized the amount of public internet

that we go through and we maximized the amount

of private AWS network that we go through.

So, transfer acceleration

is a great way to speed up transfers.

Okay, now how about getting files?

How about reading the file in the most efficient way?

We have something called an S3 Byte Range Fetches,

and so it is to paralyze Gets,

by getting specific byte ranges for your files.

So, it's also in case you have a failure

to get a specific byte range,

then you can retry a smaller byte range

and you have better resilience in case of failures.

So, it can be used to speed up downloads this time.

So, let's try a file in S3, it's really, really big

and this is the file.

Maybe you want to request the first part,

which is the first few bytes of the file,

then the second part and then the end parts.

So, we request all these parts

as specific bytes range fetches,

that's why it's called byte range,

because we only request a specific range of the file.

And all these requests can be made in parallel.

So, the idea is that we can parallelize the Gets

and speed up the downloads.

The second use case is to only retrieve

a partial amount of the file.

For example, if you know that the first 50 bytes

of the file in S3 are a header

and give you some information about the file,

then you can just issue a header request

to byte range request for the headers

using the first say 50 bytes,

and you would get that information very quickly.

All right, so that's it for S3 performance.

We've seen how to speed up uploads-downloads.

We've seen the baseline performance

and we've seen the KMS limits.

So, make sure you know those going into the exam

and I will see you in the next lecture.